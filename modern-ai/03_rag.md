# Retrieval-Augmented Generation (RAG)

Retrieval-Augmented Generation (RAG) enhances LLMs by combining them with external data sources. Instead of relying solely on a model's internal knowledge, RAG enables dynamic access to updated or domain-specific content during generation.

## Learning Objectives

By the end of this module, you should be able to:

- Understand the RAG architecture and its components (retriever + generator)
- Build a simple RAG pipeline using vector databases and LLMs
- Fine-tune retrieval systems with embeddings
- Apply RAG to real-world use cases (Q&A, chatbots, knowledge assistants)

## Why Itâ€™s Important

LLMs have limited context windows and static training data. RAG solves this by letting the model *fetch* relevant information at inference time, making it suitable for up-to-date, factual, or enterprise-specific AI systems.


## Core Topics & Resources

### 1. RAG Fundamentals

- [Retrieval-Augmented Generation â€“ YouTube (AssemblyAI)](https://www.youtube.com/watch?v=qbIk7-JPB2c) â€” **Free**
- [Hugging Face RAG Architecture Overview](https://huggingface.co/blog/rag) â€” **Free**
- [RAG Explained â€“ Zilliz](https://zilliz.com/blog/retrieval-augmented-generation-explained) â€” **Free**

Covers:
- Vector stores, retrievers, and generators
- How RAG differs from fine-tuning
- Overview of in-memory vs. persistent RAG systems

### 2. Building a RAG Pipeline

- [LangChain RAG Tutorial](https://docs.langchain.com/docs/use-cases/question-answering/) â€” **Free**
- [Haystack RAG Implementation](https://docs.haystack.deepset.ai/docs/rag) â€” **Free**
- [ChromaDB + OpenAI Demo](https://www.youtube.com/watch?v=GkZrTgV0nrw) â€” **Free**

Covers:
- Creating and using vector stores (Chroma, FAISS, Weaviate)
- Embedding models (OpenAI, Hugging Face, SentenceTransformers)
- Chunking, retrieval, and prompt integration

### 3. Embeddings & Vector Databases

- [OpenAI Embedding Guide](https://platform.openai.com/docs/guides/embeddings) â€” **Free tier available**
- [Intro to FAISS â€“ Facebook AI](https://github.com/facebookresearch/faiss) â€” **Free**
- [ChromaDB Quickstart](https://docs.trychroma.com/) â€” **Free**

Covers:
- Text chunking, embedding creation
- Storing and searching vectors
- Semantic similarity vs. keyword search

### 4. Evaluation & Optimization

- [RAG Best Practices â€“ Cohere](https://docs.cohere.com/docs/rag-best-practices) â€” **Free**
- [Prompt Engineering for RAG](https://www.promptingguide.ai/techniques/rag) â€” **Free**
- [Context Re-ranking with BGE/ColBERT](https://huggingface.co/spaces/mteb/leaderboard) â€” **Free**

Covers:
- Retrieval precision and recall
- Re-ranking strategies
- Optimizing chunk size, temperature, prompt context

## Suggested Projects & Practice

- Build a "Chat with Docs" tool using LangChain and FAISS
- Create a RAG-powered knowledge assistant for your notes or PDFs
- Compare keyword vs. semantic search performance in a retrieval setup
- Use Haystack or Chroma to deploy a RAG Q&A bot

## Checkpoint: Before You Move On

You should now be able to:

- Explain how RAG works and why it's used
- Build a full RAG pipeline from documents to query output
- Use vector databases and embeddings effectively
- Optimize RAG for relevance and performance

ðŸ”— Next: [Prompt Engineering â†’](./04_prompt-engineering.md)
